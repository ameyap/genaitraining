{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an MCP Server for Tool Integration",
    "",
    "This notebook demonstrates how to build a simple Model Context Protocol (MCP) server that provides several useful tools for LLMs. You'll learn how to create an MCP server from scratch, implement multiple tool types, and connect them to LLMs.",
    "",
    "## Learning Objectives",
    "",
    "- Understand the structure of an MCP server",
    "- Implement various tool types (data retrieval, computation, external APIs)",
    "- Define tool schemas using JSONSchema",
    "- Configure proper authentication and security",
    "- Test tool integrations with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup",
    "",
    "First, let's install the necessary packages to implement our MCP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages",
    "!pip install fastapi uvicorn requests python-dotenv openai langchain"  
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os",
    "import json",
    "import requests",
    "from typing import Dict, List, Any, Optional",
    "from dotenv import load_dotenv",
    "from fastapi import FastAPI, HTTPException, Depends, Header, Request",
    "from pydantic import BaseModel, Field",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the Model Context Protocol",
    "",
    "The Model Context Protocol (MCP) is a standardized way for LLMs to interact with external tools. It provides a consistent interface for tool discovery, invocation, and response handling.",
    "",
    "Key components of an MCP server:",
    "",
    "1. **Tool Registry**: Catalogs available tools and their schemas",
    "2. **Request Handler**: Processes incoming tool calls",
    "3. **Tool Implementation**: The actual business logic of each tool",
    "4. **Response Formatter**: Structures outputs in a format the LLM can understand",
    "5. **Authentication Layer**: Secures access to the tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining Our Tools",
    "",
    "We'll implement three different tools to demonstrate the versatility of MCP:",
    "",
    "1. **Weather Tool**: Retrieves current weather information for a location",
    "2. **Calculator Tool**: Performs mathematical operations",
    "3. **Knowledge Base Tool**: Searches a simple knowledge base for information",
    "",
    "Let's define the schemas for these tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tool schemas",
    "TOOL_REGISTRY = {",
    "    "weather": {",
    "        "description": "Get current weather for a location",",
    "        "parameters": {",
    "            "type": "object",",
    "            "properties": {",
    "                "location": {",
    "                    "type": "string",",
    "                    "description": "City name or location"",
    "                }",
    "            },",
    "            "required": ["location"]",
    "        }",
    "    },",
    "    ",
    "    "calculator": {",
    "        "description": "Perform a mathematical calculation",",
    "        "parameters": {",
    "            "type": "object",",
    "            "properties": {",
    "                "expression": {",
    "                    "type": "string",",
    "                    "description": "Mathematical expression to evaluate (e.g., '2 + 2', '5 * 10')"",
    "                }",
    "            },",
    "            "required": ["expression"]",
    "        }",
    "    },",
    "    ",
    "    "knowledge_base": {",
    "        "description": "Search the knowledge base for information",",
    "        "parameters": {",
    "            "type": "object",",
    "            "properties": {",
    "                "query": {",
    "                    "type": "string",",
    "                    "description": "Search query"",
    "                },",
    "                "max_results": {",
    "                    "type": "integer",",
    "                    "description": "Maximum number of results to return",",
    "                    "default": 3",
    "                }",
    "            },",
    "            "required": ["query"]",
    "        }",
    "    }",
    "}",
    "",
    "print("Tool schemas defined!")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing Tool Logic",
    "",
    "Now let's implement the actual functionality of each tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather tool implementation",
    "async def get_weather(location: str) -> Dict[str, Any]:",
    "    \"\"\"",
    "    Simulate a weather API call. In a real application, this would call an external weather API.",
    "    \"\"\"",
    "    # This is a simulated response. In a real implementation, you would call a weather API.",
    "    weather_data = {",
    "        "New York": {"temperature": 72, "conditions": "sunny", "humidity": 45},",
    "        "London": {"temperature": 62, "conditions": "rainy", "humidity": 80},",
    "        "Tokyo": {"temperature": 78, "conditions": "partly cloudy", "humidity": 60},",
    "        "Sydney": {"temperature": 82, "conditions": "clear", "humidity": 40},",
    "    }",
    "    ",
    "    # Default response if location not found",
    "    default_data = {"temperature": 70, "conditions": "unknown", "humidity": 50}",
    "    ",
    "    # Get data for the requested location or return the default",
    "    result = weather_data.get(location, default_data)",
    "    result["location"] = location  # Add the location to the response",
    "    ",
    "    return result",
    "",
    "# Calculator tool implementation",
    "async def calculate(expression: str) -> Dict[str, Any]:",
    "    \"\"\"",
    "    Evaluate a mathematical expression.",
    "    \"\"\"",
    "    try:",
    "        # Use eval with restricted globals to only allow safe operations",
    "        allowed_names = {",
    "            "abs": abs,",
    "            "max": max,",
    "            "min": min,",
    "            "sum": sum,",
    "            "round": round",
    "        }",
    "        ",
    "        # Evaluate the expression using Python's eval function",
    "        # Note: In a production environment, you would want to use a safer method",
    "        result = eval(expression, {"__builtins__": {}}, allowed_names)",
    "        ",
    "        return {",
    "            "expression": expression,",
    "            "result": result",
    "        }",
    "    except Exception as e:",
    "        return {",
    "            "expression": expression,",
    "            "error": str(e)",
    "        }",
    "",
    "# Knowledge base tool implementation",
    "async def search_knowledge_base(query: str, max_results: int = 3) -> Dict[str, Any]:",
    "    \"\"\"",
    "    Search a simulated knowledge base for information.",
    "    \"\"\"",
    "    # Simulated knowledge base",
    "    knowledge_base = [",
    "        {",
    "            "title": "Model Context Protocol Overview",",
    "            "content": "MCP is an open protocol for standardizing how AI assistants connect to external systems."",
    "        },",
    "        {",
    "            "title": "Tool Schema Definition",",
    "            "content": "Tool schemas in MCP are defined using JSONSchema, describing parameters and return types."",
    "        },",
    "        {",
    "            "title": "MCP Security Best Practices",",
    "            "content": "Always implement authentication, input validation, and rate limiting in your MCP servers."",
    "        },",
    "        {",
    "            "title": "Tool Types",",
    "            "content": "Common tool types include data retrieval, computation, and integration with external services."",
    "        },",
    "        {",
    "            "title": "MCP Request Format",",
    "            "content": "MCP requests include a tool name and parameters conforming to the tool's schema."",
    "        }",
    "    ]",
    "    ",
    "    # Simple search implementation",
    "    results = []",
    "    for item in knowledge_base:",
    "        if query.lower() in item["title"].lower() or query.lower() in item["content"].lower():",
    "            results.append(item)",
    "        ",
    "        if len(results) >= max_results:",
    "            break",
    "    ",
    "    return {",
    "        "query": query,",
    "        "results": results,",
    "        "total_results": len(results)",
    "    }",
    "",
    "print("Tool implementations defined!")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating the MCP Server",
    "",
    "Now we'll create a FastAPI application that serves as our MCP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define request and response models",
    "class ToolCallRequest(BaseModel):",
    "    name: str",
    "    parameters: Dict[str, Any]",
    "",
    "class ToolCallResponse(BaseModel):",
    "    status: str",
    "    result: Optional[Dict[str, Any]] = None",
    "    error: Optional[str] = None",
    "",
    "# Create FastAPI application",
    "app = FastAPI(title="MCP Tool Server")",
    "",
    "# Simple API key authentication for demo purposes",
    "API_KEY = "demo-mcp-key"  # In a real app, use secure key management",
    "",
    "async def verify_api_key(api_key: str = Header(None)):",
    "    if api_key != API_KEY:",
    "        raise HTTPException(status_code=401, detail="Invalid API key")",
    "    return api_key",
    "",
    "# Tool discovery endpoint",
    "@app.get("/mcp/v1/tools")",
    "async def list_tools(_: str = Depends(verify_api_key)):",
    "    return {"tools": TOOL_REGISTRY}",
    "",
    "# Tool calling endpoint",
    "@app.post("/mcp/v1/tools", response_model=ToolCallResponse)",
    "async def handle_tool_call(",
    "    request: ToolCallRequest,",
    "    _: str = Depends(verify_api_key)",
    "):",
    "    tool_name = request.name",
    "    parameters = request.parameters",
    "    ",
    "    # Check if the requested tool exists",
    "    if tool_name not in TOOL_REGISTRY:",
    "        return ToolCallResponse(",
    "            status="error",",
    "            error=f"Tool {tool_name} not found"",
    "        )",
    "    ",
    "    try:",
    "        # Call the appropriate tool function based on the tool name",
    "        if tool_name == "weather":",
    "            result = await get_weather(parameters.get("location"))",
    "        elif tool_name == "calculator":",
    "            result = await calculate(parameters.get("expression"))",
    "        elif tool_name == "knowledge_base":",
    "            result = await search_knowledge_base(",
    "                parameters.get("query"),",
    "                parameters.get("max_results", 3)",
    "            )",
    "        else:",
    "            return ToolCallResponse(",
    "                status="error",",
    "                error="Tool implementation not found"",
    "            )",
    "        ",
    "        return ToolCallResponse(",
    "            status="success",",
    "            result=result",
    "        )",
    "        ",
    "    except Exception as e:",
    "        return ToolCallResponse(",
    "            status="error",",
    "            error=str(e)",
    "        )",
    "",
    "print("MCP server defined!")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Running the MCP Server",
    "",
    "To run the server, you would typically execute the following command in a terminal:",
    "",
    "```bash",
    "uvicorn mcp_server:app --reload",
    "```",
    "",
    "For the purpose of this notebook, let's see how you would test the server directly. In a real scenario, the server would be running as a separate process or service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell demonstrates how to make direct calls to the tool functions for testing",
    "import asyncio",
    "",
    "# Test the weather tool",
    "async def test_weather():",
    "    print("Testing weather tool...")",
    "    result = await get_weather("Tokyo")",
    "    print(f"Weather in Tokyo: {result}")",
    "    ",
    "# Test the calculator tool",
    "async def test_calculator():",
    "    print("\nTesting calculator tool...")",
    "    result = await calculate("5 * (10 + 2)")",
    "    print(f"Calculation result: {result}")",
    "",
    "# Test the knowledge base tool",
    "async def test_kb():",
    "    print("\nTesting knowledge base tool...")",
    "    result = await search_knowledge_base("security", 2)",
    "    print(f"Knowledge base results:\n{json.dumps(result, indent=2)}")",
    "",
    "# Run all tests",
    "await test_weather()",
    "await test_calculator()",
    "await test_kb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Connecting an LLM to Your MCP Server",
    "",
    "Now let's see how to connect an LLM to use our MCP server tools. We'll use OpenAI's API as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This demonstrates how an LLM would use our MCP tools",
    "# Note: Set your OpenAI API key before running this cell",
    "# os.environ['OPENAI_API_KEY'] = 'your-api-key-here'  # Uncomment and set your key",
    "# In this example, we're directly calling the tool functions rather than using HTTP",
    "",
    "def run_conversation():",
    "    try:",
    "        client = OpenAI()",
    "        ",
    "        # Define available tools",
    "        tools = [",
    "            {",
    "                "type": "function",",
    "                "function": {",
    "                    "name": "weather",",
    "                    "description": "Get current weather for a location",",
    "                    "parameters": {",
    "                        "type": "object",",
    "                        "properties": {",
    "                            "location": {",
    "                                "type": "string",",
    "                                "description": "City name or location"",
    "                            }",
    "                        },",
    "                        "required": ["location"]",
    "                    }",
    "                }",
    "            },",
    "            {",
    "                "type": "function",",
    "                "function": {",
    "                    "name": "calculator",",
    "                    "description": "Perform a mathematical calculation",",
    "                    "parameters": {",
    "                        "type": "object",",
    "                        "properties": {",
    "                            "expression": {",
    "                                "type": "string",",
    "                                "description": "Mathematical expression"",
    "                            }",
    "                        },",
    "                        "required": ["expression"]",
    "                    }",
    "                }",
    "            },",
    "        ]",
    "        ",
    "        # Step 1: Send the user query and available tools to the model",
    "        messages = [",
    "            {"role": "system", "content": "You are a helpful assistant that can use tools to answer questions."},",
    "            {"role": "user", "content": "What's the weather in London? Also, calculate 25 * 4."}",
    "        ]",
    "        ",
    "        print("Sending initial request to the model...")",
    "        response = client.chat.completions.create(",
    "            model="gpt-4",  # or another model that supports tools",
    "            messages=messages,",
    "            tools=tools,",
    "            tool_choice="auto"",
    "        )",
    "        assistant_message = response.choices[0].message",
    "        messages.append(assistant_message)",
    "        ",
    "        print("\nAssistant is considering tools to use...")",
    "        ",
    "        # Process any tool calls",
    "        tool_calls = assistant_message.tool_calls",
    "        if tool_calls:",
    "            print("\nAssistant decided to use tools:")",
    "            for tool_call in tool_calls:",
    "                function_name = tool_call.function.name",
    "                function_args = json.loads(tool_call.function.arguments)",
    "                ",
    "                print(f"Tool call: {function_name} with args: {function_args}")",
    "                ",
    "                # Execute the appropriate tool",
    "                function_response = None",
    "                if function_name == "weather":",
    "                    function_response = asyncio.run(get_weather(function_args.get("location")))",
    "                elif function_name == "calculator":",
    "                    function_response = asyncio.run(calculate(function_args.get("expression")))",
    "                else:",
    "                    function_response = {"error": "Unknown tool"}",
    "                ",
    "                # Append the tool response to messages",
    "                messages.append(",
    "                    {",
    "                        "tool_call_id": tool_call.id,",
    "                        "role": "tool",",
    "                        "name": function_name,",
    "                        "content": json.dumps(function_response)",
    "                    }",
    "                )",
    "            ",
    "            # Send the tool results back to the model",
    "            print("\nSending tool results back to the model...")",
    "            second_response = client.chat.completions.create(",
    "                model="gpt-4",",
    "                messages=messages,",
    "            )",
    "            assistant_message = second_response.choices[0].message",
    "            messages.append(assistant_message)",
    "            ",
    "            print("\nFinal response:")",
    "            print(assistant_message.content)",
    "            ",
    "        else:",
    "            print("\nAssistant didn't use any tools. Response:")",
    "            print(assistant_message.content)",
    "            ",
    "        return messages",
    "        ",
    "    except Exception as e:",
    "        print(f"Error: {e}")",
    "        print("Note: To run this example, you need an OpenAI API key with access to a model that supports tools")",
    "    ",
    "# Commenting out the execution for this demo - uncomment to test with your API key",
    "# run_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Securing Your MCP Server",
    "",
    "Security is crucial for MCP servers, especially when they provide access to sensitive operations. Here are key security considerations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    """""Security best practices for MCP servers:",
    "",
    "1. Authentication:",
    "   - Use strong API key validation or OAuth2",
    "   - Implement token-based authentication",
    "   - Consider using HTTPS client certificates for high-security systems",
    "",
    "2. Authorization:",
    "   - Implement role-based access control (RBAC)",
    "   - Create tiered access levels for different tools",
    "   - Apply principle of least privilege",
    "",
    "3. Input Validation:",
    "   - Validate all input parameters against schemas",
    "   - Implement strict type checking",
    "   - Sanitize inputs to prevent injection attacks",
    "",
    "4. Rate Limiting:",
    "   - Implement request rate limiting",
    "   - Add usage quotas per API key",
    "   - Monitor for abuse patterns",
    "",
    "5. Audit Logging:",
    "   - Log all tool invocations",
    "   - Include request details, timestamps, and caller information",
    "   - Consider privacy implications of logs",
    "",
    "6. Error Handling:",
    "   - Use generic error messages to clients",
    "   - Implement detailed internal logging",
    "   - Never expose stack traces to clients",
    "",
    "7. TLS/HTTPS:",
    "   - Always use HTTPS for all endpoints",
    "   - Keep TLS certificates up to date",
    "   - Use strong cipher suites",
    """""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices for Tool Design",
    "",
    "When designing tools for your MCP server, consider these best practices:",
    "",
    "1. **Clear Single-Purpose Tools**: Design each tool to do one thing well",
    "2. **Comprehensive Parameter Descriptions**: Make argument descriptions clear and detailed",
    "3. **Error Handling**: Include robust error handling with informative messages",
    "4. **Documentation**: Provide examples for each tool",
    "5. **Idempotency**: Make tools safe to call multiple times when applicable",
    "6. **Response Structure**: Return well-structured, consistent responses",
    "7. **Timeouts**: Implement proper timeouts for external services",
    "8. **Progress Indicators**: For long-running operations, provide status endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion",
    "",
    "In this notebook, we've created a simple MCP server with three different tools and demonstrated how to connect it to an LLM. Key takeaways:",
    "",
    "- MCP provides a standardized way for LLMs to interact with external tools",
    "- Tools can provide data retrieval, computational capabilities, or external service integration",
    "- Proper tool design with clear schemas improves LLM interactions",
    "- Security is a critical aspect of MCP server implementation",
    "",
    "From here, you could expand your MCP server by:",
    "",
    "1. Adding more sophisticated tools",
    "2. Implementing real external API integrations",
    "3. Enhancing security features",
    "4. Deploying the server as a production service",
    "5. Building tool discovery and registration mechanisms"
   ]
  }
 ]
}