{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic LLM Interaction",
    "",
    "This notebook demonstrates fundamental methods for interacting with large language models (LLMs), including loading models, crafting effective prompts, tuning parameters, and processing responses.",
    "",
    "## Learning Objectives",
    "",
    "- Configure and initialize language models",
    "- Format prompts for effective results",
    "- Adjust generation parameters like temperature, top-p, and top-k",
    "- Parse and process model responses",
    "- Compare outputs across different parameter settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup",
    "",
    "First, we'll install and import the necessary libraries. We'll use the Hugging Face Transformers library to interact with models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages",
    "!pip install transformers torch accelerate"  
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch",
    "from transformers import AutoModelForCausalLM, AutoTokenizer",
    "import time",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Loading",
    "",
    "Let's load a small open-source LLM that can run on a standard notebook environment. For this example, we'll use TinyLlama, which is a compact model that demonstrates key concepts without requiring extensive resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"",
    "",
    "# Load tokenizer and model",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")",
    "",
    "print(f\"Model loaded: {model_name}\")",
    "print(f\"Model parameters: {model.num_parameters() / 1_000_000:.1f} million\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt Formatting",
    "",
    "Different models expect different prompt formats. For many instruction-tuned models, including a system message and formatting the user query properly can significantly improve results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(user_input, system_message=\"You are a helpful AI assistant.\"):",
    "    # Format for chat-tuned models",
    "    prompt = f\"<|system|>\\n{system_message}\\n<|user|>\\n{user_input}\\n<|assistant|>\\n\"",
    "    return prompt",
    "",
    "# Example formatting",
    "test_prompt = format_prompt(\"What are three key concepts in machine learning?\")",
    "print(\"Formatted prompt:\")",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Generation with Default Parameters",
    "",
    "Let's try generating text with default parameters first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_new_tokens=256, **kwargs):",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)",
    "    start_time = time.time()",
    "    ",
    "    # Generate with provided parameters",
    "    outputs = model.generate(",
    "        inputs.input_ids,",
    "        max_new_tokens=max_new_tokens,",
    "        **kwargs",
    "    )",
    "    ",
    "    # Get generation time",
    "    gen_time = time.time() - start_time",
    "    ",
    "    # Decode response, removing prompt",
    "    prompt_length = inputs.input_ids.shape[1]",
    "    response = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)",
    "    ",
    "    return response, gen_time",
    "",
    "# Generate with default parameters",
    "user_question = \"Explain how tokenization works in language models.\"",
    "prompt = format_prompt(user_question)",
    "response, gen_time = generate_response(prompt)",
    "",
    "print(f\"User question: {user_question}\\n\")",
    "print(f\"Response (generated in {gen_time:.2f}s):\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parameter Tuning",
    "",
    "Now let's explore how different parameters affect generation.",
    "",
    "### 4.1 Temperature",
    "",
    "Temperature controls randomness: lower values (e.g., 0.3) make the output more deterministic, while higher values (e.g., 1.2) make it more random and creative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Write a short poem about artificial intelligence.\"",
    "prompt = format_prompt(question)",
    "",
    "print(\"Low Temperature (0.3) - More focused, deterministic:\\n\")",
    "response_low_temp, _ = generate_response(prompt, temperature=0.3)",
    "print(response_low_temp)",
    "",
    "print(\"\\nMedium Temperature (0.7) - Balanced:\\n\")",
    "response_med_temp, _ = generate_response(prompt, temperature=0.7)",
    "print(response_med_temp)",
    "",
    "print(\"\\nHigh Temperature (1.2) - More random, creative:\\n\")",
    "response_high_temp, _ = generate_response(prompt, temperature=1.2)",
    "print(response_high_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Top-p (Nucleus) Sampling",
    "",
    "Top-p sampling selects from the smallest possible set of tokens whose cumulative probability exceeds the probability p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Suggest three creative names for a tech startup focused on AI education.\"",
    "prompt = format_prompt(question)",
    "",
    "print(\"Low Top-p (0.5) - More focused on likely tokens:\\n\")",
    "response_low_p, _ = generate_response(prompt, temperature=0.7, top_p=0.5)",
    "print(response_low_p)",
    "",
    "print(\"\\nHigh Top-p (0.95) - Considers more token diversity:\\n\")",
    "response_high_p, _ = generate_response(prompt, temperature=0.7, top_p=0.95)",
    "print(response_high_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Top-k Sampling",
    "",
    "Top-k sampling restricts the model to only consider the k most likely tokens at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Generate a short marketing tagline for an AI-powered productivity tool.\"",
    "prompt = format_prompt(question)",
    "",
    "print(\"Low Top-k (10) - Limited options:\\n\")",
    "response_low_k, _ = generate_response(prompt, temperature=0.7, top_k=10)",
    "print(response_low_k)",
    "",
    "print(\"\\nHigh Top-k (50) - More options:\\n\")",
    "response_high_k, _ = generate_response(prompt, temperature=0.7, top_k=50)",
    "print(response_high_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Response Parsing",
    "",
    "For structured outputs, we can guide the model to generate responses in a specific format (like JSON) and then parse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json",
    "import re",
    "",
    "def extract_json(text):",
    "    \"\"\"Extract JSON from text if present\"\"\"",
    "    # Look for JSON pattern surrounded by ``` or ```json",
    "    json_match = re.search(r'```(?:json)?\\s*({[^`]*})\\s*```', text)",
    "    if json_match:",
    "        json_str = json_match.group(1)",
    "        try:",
    "            return json.loads(json_str)",
    "        except json.JSONDecodeError:",
    "            pass",
    "    ",
    "    # Try to find any JSON-like structure",
    "    json_match = re.search(r'({\\s*"[^"]*"\\s*:[^}]*})', text)",
    "    if json_match:",
    "        json_str = json_match.group(1)",
    "        try:",
    "            return json.loads(json_str)",
    "        except json.JSONDecodeError:",
    "            pass",
    "            ",
    "    return None",
    "",
    "# Try to get structured output",
    "question = \"List three popular LLMs with their sizes in JSON format.\"",
    "prompt = format_prompt(question + \" Return the information as a JSON object with model names as keys and sizes in billions of parameters as values.\")",
    "response, _ = generate_response(prompt, temperature=0.3)  # Lower temperature for more structured output",
    "print(\"Response:\\n\")",
    "print(response)",
    "",
    "# Try to parse JSON",
    "json_data = extract_json(response)",
    "if json_data:",
    "    print(\"\\nParsed JSON:\")",
    "    print(json.dumps(json_data, indent=2))",
    "else:",
    "    print(\"\\nCouldn't extract valid JSON.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Context Window Experiment",
    "",
    "Let's explore how the model handles prompts of different lengths within its context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get token count",
    "def count_tokens(text):",
    "    return len(tokenizer.encode(text))",
    "",
    "# Short prompt",
    "short_prompt = format_prompt(\"Summarize the key ideas of deep learning.\")",
    "print(f\"Short prompt token count: {count_tokens(short_prompt)}\")",
    "",
    "# Medium prompt with some context",
    "medium_context = \"Deep learning has revolutionized AI in the last decade. It uses neural networks with multiple layers to learn from data. These models have achieved remarkable results in various domains including computer vision, natural language processing, and reinforcement learning. Deep learning relies on backpropagation to update weights through gradient descent.\"",
    "medium_prompt = format_prompt(f\"Using this context: '{medium_context}', explain what deep learning is to a high school student.\")",
    "print(f\"Medium prompt token count: {count_tokens(medium_prompt)}\")",
    "",
    "# Run medium example",
    "response_medium, _ = generate_response(medium_prompt, max_new_tokens=150)",
    "print(\"\\nResponse to medium prompt:\\n\")",
    "print(response_medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Putting It All Together",
    "",
    "Let's create an optimized prompting function that combines several best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_llm_query(question, system_message=\"You are a helpful AI assistant.\", temperature=0.7, max_tokens=256):",
    "    \"\"\"Complete function for interacting with LLMs using best practices\"\"\"",
    "    # Format the prompt",
    "    prompt = format_prompt(question, system_message)",
    "    ",
    "    # Check token count",
    "    token_count = count_tokens(prompt)",
    "    print(f\"Input tokens: {token_count}\")",
    "    ",
    "    # Generate with optimized parameters",
    "    response, gen_time = generate_response(",
    "        prompt,",
    "        max_new_tokens=max_tokens,",
    "        temperature=temperature,",
    "        top_p=0.9,  # Good balance for most use cases",
    "        repetition_penalty=1.1  # Slightly discourage repetition",
    "    )",
    "    ",
    "    # Calculate generation speed",
    "    output_tokens = count_tokens(response)",
    "    tokens_per_second = output_tokens / gen_time if gen_time > 0 else 0",
    "    ",
    "    print(f\"Output tokens: {output_tokens}\")",
    "    print(f\"Generation time: {gen_time:.2f} seconds\")",
    "    print(f\"Speed: {tokens_per_second:.1f} tokens/second\")",
    "    ",
    "    return response",
    "",
    "# Example query with role-specific system message",
    "system = \"You are an expert in machine learning and AI education. Provide clear, accurate, and beginner-friendly explanations.\"",
    "",
    "response = optimized_llm_query(",
    "    \"What's the difference between supervised and unsupervised learning? Give me some examples of each.\",",
    "    system_message=system",
    ")",
    "",
    "print(\"\\nFinal response:\\n\")",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion",
    "",
    "In this notebook, we've explored the fundamentals of working with LLMs including:",
    "- Loading and initializing models",
    "- Formatting effective prompts",
    "- Understanding and tuning generation parameters",
    "- Parsing and processing model responses",
    "- Optimizing for different use cases",
    "",
    "For production applications, you would typically use hosted API services like OpenAI, Anthropic, or AWS Bedrock instead of running models locally, but the same principles apply.",
    "",
    "### Next Steps",
    "",
    "1. Experiment with different prompt templates",
    "2. Try larger models if your hardware supports them",
    "3. Explore more complex parameter combinations",
    "4. Build a simple application that leverages these techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 }
}