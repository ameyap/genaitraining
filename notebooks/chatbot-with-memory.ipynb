{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with Memory Implementation",
    "",
    "This notebook demonstrates how to build a conversational AI system with sophisticated memory management for contextual interactions. We'll explore different memory types and strategies for effective conversation handling.",
    "",
    "## Learning Objectives",
    "",
    "- Implement different types of conversational memory",
    "- Manage conversation history within context window constraints",
    "- Create a system that maintains coherent conversation flow",
    "- Build strategic memory management for extended conversations",
    "- Develop techniques for maintaining consistent personality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup",
    "",
    "Let's install the necessary packages for building our chatbot with memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages",
    "!pip install langchain langchain_openai chromadb"  
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os",
    "import uuid",
    "from langchain_openai import ChatOpenAI",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory",
    "from langchain.memory import ConversationSummaryMemory, ConversationSummaryBufferMemory",
    "from langchain.chains import ConversationChain",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage",
    "from IPython.display import display, Markdown"  
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the Model",
    "",
    "We'll initialize our LLM to power the chatbot. For this example, we'll use OpenAI's GPT model, but you could substitute this with another model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key here or use environment variables",
    "# os.environ['OPENAI_API_KEY'] = 'your-api-key'  # Uncomment and replace",
    "",
    "# Initialize the LLM",
    "llm = ChatOpenAI(",
    "    model_name='gpt-3.5-turbo',",
    "    temperature=0.7",
    ")",
    "",
    "print("Model initialized")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Conversation Memory",
    "",
    "Let's start with the most basic form of memory: a conversation buffer that keeps all past exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a basic conversation memory",
    "buffer_memory = ConversationBufferMemory()",
    "",
    "# Initialize the conversation chain",
    "conversation = ConversationChain(",
    "    llm=llm,",
    "    memory=buffer_memory,",
    "    verbose=True",
    ")",
    "",
    "# Let's have a simple conversation",
    "response1 = conversation.predict(input="Hi! My name is Alice.")",
    "print("Response 1:", response1)"]
  }
 ]
}