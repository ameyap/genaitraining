{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit UI Implementation for GenAI Applications",
    "",
    "This notebook demonstrates how to build user-friendly interfaces for GenAI applications using Streamlit. We'll create a complete application with interactive components, dynamic content rendering, and proper state management.",
    "",
    "## Learning Objectives",
    "",
    "- Set up a basic Streamlit application structure",
    "- Create interactive input components for user interaction",
    "- Implement dynamic content rendering for AI-generated outputs",
    "- Manage application state effectively",
    "- Prepare the application for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup",
    "",
    "First, let's install the necessary packages to work with Streamlit and LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages",
    "!pip install streamlit langchain openai python-dotenv"  
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Streamlit Application Structure",
    "",
    "We'll start by examining the core structure of a Streamlit application. Since Streamlit needs to run as a script, we'll save this code to an external file called `app.py`.",
    "",
    "Let's look at the basic structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py",
    "import streamlit as st",
    "import os",
    "from dotenv import load_dotenv",
    "from langchain_openai import ChatOpenAI",
    "from langchain.schema import HumanMessage, SystemMessage",
    "",
    "# Load environment variables",
    "load_dotenv()",
    "",
    "# Page configuration",
    "st.set_page_config(",
    "    page_title="GenAI Assistant",",
    "    page_icon="ðŸ¤–",",
    "    layout="wide",",
    ")",
    "",
    "# Application title",
    "st.title("ðŸ¤– GenAI Assistant")",
    "st.markdown("""A simple AI assistant powered by LLMs. Ask questions, get creative responses, ",
    "or generate content for your projects.""")",
    "",
    "# Initialize session state for chat history",
    "if 'messages' not in st.session_state:",
    "    st.session_state.messages = []",
    "",
    "# Main function",
    "def main():",
    "    # Sidebar for configuration",
    "    with st.sidebar:",
    "        st.header("Configuration")",
    "        model = st.selectbox(",
    "            "Select Model",",
    "            ["gpt-3.5-turbo", "gpt-4"],",
    "            index=0",
    "        )",
    "        temperature = st.slider("Temperature", min_value=0.0, max_value=1.0, value=0.7, step=0.1)",
    "        system_prompt = st.text_area(",
    "            "System Prompt",",
    "            value="You are a helpful AI assistant. Provide clear, concise, and accurate responses.",",
    "            height=100",
    "        )",
    "    ",
    "    # Initialize the LLM",
    "    llm = ChatOpenAI(model_name=model, temperature=temperature)",
    "    ",
    "    # Display chat history",
    "    for message in st.session_state.messages:",
    "        with st.chat_message(message["role"]):",
    "            st.markdown(message["content"])",
    "    ",
    "    # Input for new messages",
    "    user_input = st.chat_input("What would you like to know?")",
    "    ",
    "    if user_input:",
    "        # Add user message to chat history",
    "        st.session_state.messages.append({"role": "user", "content": user_input})",
    "        with st.chat_message("user"):",
    "            st.markdown(user_input)",
    "        ",
    "        # Display assistant thinking indicator",
    "        with st.chat_message("assistant"):",
    "            message_placeholder = st.empty()",
    "            message_placeholder.markdown("Thinking...")",
    "            ",
    "            # Prepare messages for the LLM",
    "            messages = [",
    "                SystemMessage(content=system_prompt)",
    "            ]",
    "            ",
    "            # Add chat history (limited to last 5 exchanges to manage context)",
    "            history = st.session_state.messages[-10:]  # Last 5 exchanges (10 messages)",
    "            for msg in history:",
    "                if msg["role"] == "user":",
    "                    messages.append(HumanMessage(content=msg["content"]))",
    "                elif msg["role"] == "assistant":",
    "                    messages.append(AIMessage(content=msg["content"]))",
    "            ",
    "            # Get response from LLM",
    "            try:",
    "                response = llm.predict_messages(messages)",
    "                response_text = response.content",
    "                message_placeholder.markdown(response_text)",
    "                st.session_state.messages.append({"role": "assistant", "content": response_text})",
    "            except Exception as e:",
    "                message_placeholder.markdown(f"Error: {str(e)}")",
    "",
    "if __name__ == "__main__":",
    "    main()"
   ]
  }
 ]
}