{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System Implementation",
    "",
    "This notebook demonstrates how to build a complete Retrieval-Augmented Generation (RAG) system using open-source tools. We'll walk through each component of the RAG pipeline, from document ingestion to response generation.",
    "",
    "**What you'll learn:**",
    "- Document processing and chunking strategies",
    "- Vector database setup and configuration",
    "- Embedding generation and storage",
    "- Query pipeline construction",
    "- Retrieval and generation integration",
    "- RAG system evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup",
    "",
    "First, let's install the required dependencies for our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages",
    "!pip install langchain langchain_community chromadb sentence-transformers tiktoken",
    "!pip install pypdf unstructured openai",
    "!pip install faiss-cpu",
    "!pip install matplotlib numpy pandas"  
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os",
    "import time",
    "from dotenv import load_dotenv",
    "import numpy as np",
    "import pandas as pd",
    "import matplotlib.pyplot as plt",
    "from typing import List, Dict, Any",
    "",
    "# LangChain imports",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter",
    "from langchain.document_loaders import PyPDFLoader, TextLoader, DirectoryLoader",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings",
    "from langchain.vectorstores import Chroma, FAISS",
    "from langchain.chat_models import ChatOpenAI",
    "from langchain.llms import OpenAI",
    "from langchain.retrievers import ContextualCompressionRetriever",
    "from langchain.retrievers.document_compressors import LLMChainExtractor",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain",
    "from langchain.memory import ConversationBufferMemory",
    "from langchain.prompts import ChatPromptTemplate",
    "",
    "# Load environment variables",
    "load_dotenv()",
    "# Set your API key via environment variable or direct assignment",
    "# os.environ['OPENAI_API_KEY'] = 'your-api-key-here'  # Uncomment if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document Processing",
    "",
    "### 1.1 Loading Documents",
    "",
    "First, let's load our documents. We'll create a sample text file, but in a real application, you might load PDFs, Word documents, or other text sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample text document",
    "with open('sample_data.txt', 'w') as f:",
    "    f.write('''"Retrieval-Augmented Generation (RAG) is a technique that enhances large language models with external knowledge.",
    "    ",
    "RAG works by retrieving relevant information from a knowledge base and injecting it into the context provided to the language model.",
    "This approach has several advantages:",
    "    ",
    "1. Improved factual accuracy: By providing the model with up-to-date and domain-specific information, RAG can significantly reduce hallucinations and factual errors.",
    "    ",
    "2. Adaptability: RAG allows models to access information that wasn't available during their training, making them more adaptable to new knowledge without requiring full retraining.",
    "    ",
    "3. Transparency: The retrieval step creates a clear link between the model's responses and the sources of information, enhancing explainability.",
    "    ",
    "4. Cost efficiency: RAG can achieve better performance on knowledge-intensive tasks without increasing the model size.",
    "    ",
    "The typical RAG pipeline consists of these main components:",
    "    ",
    "- Document processing: Breaking down documents into manageable chunks",
    "- Embedding generation: Converting text chunks into vector representations",
    "- Vector storage: Efficiently storing and retrieving vectors using vector databases",
    "- Retrieval: Finding the most relevant information based on query similarity",
    "- Generation: Producing responses using retrieved context and the original query",
    "    ",
    "Common challenges in RAG implementations include context length limitations, relevance tuning, and handling of conflicting information.''')",
    "",
    "# Load the document",
    "loader = TextLoader('sample_data.txt')",
    "documents = loader.load()",
    "print(f"Loaded {len(documents)} document(s)")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Text Splitting Strategies",
    "",
    "Now let's split our documents into smaller chunks for more effective retrieval. We'll explore different chunking strategies and their impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different text splitters for comparison",
    "small_chunk_splitter = RecursiveCharacterTextSplitter(",
    "    chunk_size=200,",
    "    chunk_overlap=20,",
    "    length_function=len,",
    "    is_separator_regex=False,",
    ")",
    "",
    "medium_chunk_splitter = RecursiveCharacterTextSplitter(",
    "    chunk_size=500,",
    "    chunk_overlap=50,",
    "    length_function=len,",
    "    is_separator_regex=False,",
    ")",
    "",
    "# Split the documents with medium-sized chunks (our default choice)",
    "splits = medium_chunk_splitter.split_documents(documents)",
    "",
    "print(f"Created {len(splits)} chunks with medium-sized splitter")",
    "print(f"Average chunk length: {sum(len(s.page_content) for s in splits) / len(splits)} characters")",
    "print("\nSample chunk:")",
    "print(splits[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vector Database Setup",
    "",
    "Next, we'll set up a vector database to store our document embeddings. We'll compare two popular options: Chroma and FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose embeddings model - using sentence-transformers for local embedding",
    "embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')",
    "",
    "# For better quality but requiring API key:",
    "# embeddings = OpenAIEmbeddings()",
    "",
    "# Create vector stores",
    "# 1. Chroma DB",
    "chroma_db = Chroma.from_documents(",
    "    documents=splits,",
    "    embedding=embeddings,",
    "    persist_directory='./chroma_db',",
    "    collection_name='rag_tutorial'",
    ")",
    "",
    "# 2. FAISS (Facebook AI Similarity Search)",
    "faiss_db = FAISS.from_documents(",
    "    documents=splits,",
    "    embedding=embeddings",
    ")",
    "",
    "# Save FAISS index for later use",
    "faiss_db.save_local('faiss_index')",
    "",
    "print("Vector stores created successfully")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the Retrieval Pipeline",
    "",
    "Now, let's create our retrieval pipeline. We'll compare different retrieval strategies and evaluate their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basic retriever from vector store",
    "retriever = faiss_db.as_retriever(search_kwargs={"k": 3})",
    "",
    "# Let's test the retriever",
    "query = "What are the main advantages of RAG?"",
    "retrieved_docs = retriever.get_relevant_documents(query)",
    "",
    "print(f"Query: {query}")",
    "print(f"Retrieved {len(retrieved_docs)} documents")",
    "print("\nFirst retrieved document:")",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Enhancing Retrieval with Contextual Compression",
    "",
    "Let's improve our retriever by adding contextual compression to filter out irrelevant parts of the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an LLM for compression - we're using OpenAI here, but you could use other models",
    "llm = OpenAI(temperature=0)",
    "",
    "# Create compressor components - this helps extract only the relevant parts of documents",
    "compressor = LLMChainExtractor.from_llm(llm)",
    "",
    "# Create enhanced retriever with compression",
    "compression_retriever = ContextualCompressionRetriever(",
    "    base_compressor=compressor,",
    "    base_retriever=retriever",
    ")",
    "",
    "# Test the enhanced retriever",
    "compressed_docs = compression_retriever.get_relevant_documents(query)",
    "",
    "print(f"Query: {query}")",
    "print(f"Retrieved {len(compressed_docs)} compressed documents")",
    "print("\nCompressed document:")",
    "print(compressed_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Integrating Retrieval with Generation",
    "",
    "Now, let's build a complete RAG system by connecting our retriever to a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM - OpenAI's chat model",
    "chat_model = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)",
    "",
    "# Create the RAG chain",
    "qa_chain = RetrievalQA.from_chain_type(",
    "    llm=chat_model,",
    "    chain_type="stuff",  # We're using the 'stuff' chain which simply stuffs all retrieved docs into context",
    "    retriever=retriever,",
    "    return_source_documents=True,",
    "    verbose=True",
    ")",
    "",
    "# Define a custom prompt template that encourages the model to use the retrieved context",
    "custom_prompt_template = """You are a helpful assistant that answers questions based on the provided context.",
    "Use only the information from the context to answer the question. If you don't know the answer based on the context, say 'I don't have enough information to answer this question.'",
    "",
    "Context: {context}",
    "",
    "Question: {question}",
    "",
    "Answer: """",
    "",
    "# Run a test query",
    "query = "How does RAG improve factual accuracy?"",
    "result = qa_chain({"query": query})",
    "",
    "print("\nQuery:", query)",
    "print("\nResponse:", result["result"])",
    "print("\nSource Documents:")",
    "for i, doc in enumerate(result["source_documents"]):",
    "    print(f"\nDocument {i+1}:")",
    "    print(doc.page_content[:200] + "...")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adding Conversation Memory",
    "",
    "Let's enhance our RAG system by adding conversational memory, allowing it to maintain context across multiple queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up memory",
    "memory = ConversationBufferMemory(",
    "    memory_key="chat_history",",
    "    return_messages=True",
    ")",
    "",
    "# Create conversational RAG chain",
    "conversational_qa = ConversationalRetrievalChain.from_llm(",
    "    llm=chat_model,",
    "    retriever=retriever,",
    "    memory=memory,",
    "    verbose=True",
    ")",
    "",
    "# First question",
    "first_query = "What is RAG?"",
    "result = conversational_qa({"question": first_query})",
    "print("\nQ1:", first_query)",
    "print("A1:", result["answer"])",
    "",
    "# Follow-up question (referencing previous context)",
    "follow_up = "What challenges does it face?"",
    "result = conversational_qa({"question": follow_up})",
    "print("\nQ2:", follow_up)",
    "print("A2:", result["answer"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating RAG Performance",
    "",
    "Finally, let's implement some basic evaluation techniques to assess the quality of our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation queries and their expected answers",
    "eval_queries = [",
    "    {"query": "What are the main components of a RAG pipeline?", "expected_keywords": ["document", "embedding", "vector", "retrieval", "generation"]},",
    "    {"query": "How does RAG improve model responses?", "expected_keywords": ["factual", "accuracy", "adaptability", "transparency", "efficiency"]},",
    "    {"query": "What are challenges in RAG implementations?", "expected_keywords": ["context", "length", "relevance", "tuning", "conflicting"]}",
    "]",
    "",
    "# Simple keyword-based evaluation function",
    "def evaluate_response(response, expected_keywords):",
    "    score = 0",
    "    response_lower = response.lower()",
    "    for keyword in expected_keywords:",
    "        if keyword.lower() in response_lower:",
    "            score += 1",
    "    return score / len(expected_keywords)",
    "",
    "# Run evaluation",
    "evaluation_results = []",
    "for eval_item in eval_queries:",
    "    query = eval_item["query"]",
    "    expected_keywords = eval_item["expected_keywords"]",
    "    ",
    "    # Get response",
    "    result = qa_chain({"query": query})",
    "    response = result["result"]",
    "    ",
    "    # Evaluate",
    "    score = evaluate_response(response, expected_keywords)",
    "    evaluation_results.append({"query": query, "score": score, "response": response})",
    "    ",
    "    print(f"Query: {query}")",
    "    print(f"Score: {score:.2f}")",
    "    print(f"Response: {response[:100]}...\n")",
    "",
    "# Visualize results",
    "plt.figure(figsize=(10, 6))",
    "plt.bar([f"Query {i+1}" for i in range(len(evaluation_results))], [r["score"] for r in evaluation_results])",
    "plt.title('RAG System Evaluation Scores')",
    "plt.ylabel('Score (0-1)')",
    "plt.ylim(0, 1)",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion",
    "",
    "In this notebook, we've built a complete RAG system from scratch, including document processing, vector storage, retrieval enhancement, and evaluation. Key takeaways:",
    "",
    "1. **Document Processing**: Chunking strategy significantly impacts retrieval quality",
    "2. **Embedding Models**: Choice of embedding model affects semantic understanding",
    "3. **Retrieval Methods**: Techniques like contextual compression can improve relevance",
    "4. **Generation Integration**: Proper prompting helps the LLM leverage retrieved context",
    "5. **Conversation Memory**: Adding history enables multi-turn interactions",
    "6. **Evaluation**: Systematic testing is crucial for RAG system improvement",
    "",
    "Next steps to explore:",
    "- Try different embedding models",
    "- Experiment with hybrid search (combining semantic and keyword search)",
    "- Implement more sophisticated evaluation metrics",
    "- Scale to larger document collections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}